Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 192, 256, 3)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 192, 256, 8)          224       ['input_1[0][0]']             
                                                                                                  
 conv2d_1 (Conv2D)           (None, 192, 256, 8)          584       ['conv2d[0][0]']              
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 96, 128, 8)           0         ['conv2d_1[0][0]']            
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 96, 128, 16)          1168      ['max_pooling2d[0][0]']       
                                                                                                  
 conv2d_3 (Conv2D)           (None, 96, 128, 16)          2320      ['conv2d_2[0][0]']            
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 48, 64, 16)           0         ['conv2d_3[0][0]']            
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 48, 64, 32)           4640      ['max_pooling2d_1[0][0]']     
                                                                                                  
 conv2d_5 (Conv2D)           (None, 48, 64, 32)           9248      ['conv2d_4[0][0]']            
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 24, 32, 32)           0         ['conv2d_5[0][0]']            
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 24, 32, 16)           4624      ['max_pooling2d_2[0][0]']     
                                                                                                  
 conv2d_7 (Conv2D)           (None, 24, 32, 16)           2320      ['conv2d_6[0][0]']            
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 12, 16, 16)           0         ['conv2d_7[0][0]']            
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 12, 16, 8)            1160      ['max_pooling2d_3[0][0]']     
                                                                                                  
 conv2d_9 (Conv2D)           (None, 12, 16, 8)            584       ['conv2d_8[0][0]']            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 6, 8, 8)              0         ['conv2d_9[0][0]']            
 g2D)                                                                                             
                                                                                                  
 bottleneck1 (Conv2D)        (None, 6, 8, 16)             1168      ['max_pooling2d_4[0][0]']     
                                                                                                  
 conv2d_transpose (Conv2DTr  (None, 12, 16, 8)            1160      ['bottleneck1[0][0]']         
 anspose)                                                                                         
                                                                                                  
 concatenate (Concatenate)   (None, 12, 16, 16)           0         ['conv2d_transpose[0][0]',    
                                                                     'conv2d_9[0][0]']            
                                                                                                  
 conv2d_10 (Conv2D)          (None, 12, 16, 8)            1160      ['concatenate[0][0]']         
                                                                                                  
 conv2d_11 (Conv2D)          (None, 12, 16, 8)            584       ['conv2d_10[0][0]']           
                                                                                                  
 conv2d_transpose_1 (Conv2D  (None, 24, 32, 16)           1168      ['conv2d_11[0][0]']           
 Transpose)                                                                                       
                                                                                                  
 concatenate_1 (Concatenate  (None, 24, 32, 32)           0         ['conv2d_transpose_1[0][0]',  
 )                                                                   'conv2d_7[0][0]']            
                                                                                                  
 conv2d_12 (Conv2D)          (None, 24, 32, 16)           4624      ['concatenate_1[0][0]']       
                                                                                                  
 conv2d_13 (Conv2D)          (None, 24, 32, 16)           2320      ['conv2d_12[0][0]']           
                                                                                                  
 conv2d_transpose_2 (Conv2D  (None, 48, 64, 32)           4640      ['conv2d_13[0][0]']           
 Transpose)                                                                                       
                                                                                                  
 concatenate_2 (Concatenate  (None, 48, 64, 64)           0         ['conv2d_transpose_2[0][0]',  
 )                                                                   'conv2d_5[0][0]']            
                                                                                                  
 conv2d_14 (Conv2D)          (None, 48, 64, 32)           18464     ['concatenate_2[0][0]']       
                                                                                                  
 conv2d_15 (Conv2D)          (None, 48, 64, 32)           9248      ['conv2d_14[0][0]']           
                                                                                                  
 conv2d_transpose_3 (Conv2D  (None, 96, 128, 16)          4624      ['conv2d_15[0][0]']           
 Transpose)                                                                                       
                                                                                                  
 concatenate_3 (Concatenate  (None, 96, 128, 32)          0         ['conv2d_transpose_3[0][0]',  
 )                                                                   'conv2d_3[0][0]']            
                                                                                                  
 conv2d_16 (Conv2D)          (None, 96, 128, 16)          4624      ['concatenate_3[0][0]']       
                                                                                                  
 conv2d_17 (Conv2D)          (None, 96, 128, 16)          2320      ['conv2d_16[0][0]']           
                                                                                                  
 conv2d_transpose_4 (Conv2D  (None, 192, 256, 8)          1160      ['conv2d_17[0][0]']           
 Transpose)                                                                                       
                                                                                                  
 concatenate_4 (Concatenate  (None, 192, 256, 16)         0         ['conv2d_transpose_4[0][0]',  
 )                                                                   'conv2d_1[0][0]']            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 192, 256, 8)          1160      ['concatenate_4[0][0]']       
                                                                                                  
 conv2d_19 (Conv2D)          (None, 192, 256, 8)          584       ['conv2d_18[0][0]']           
                                                                                                  
 conv2d_20 (Conv2D)          (None, 192, 256, 1)          9         ['conv2d_19[0][0]']           
                                                                                                  
==================================================================================================
Total params: 85889 (335.50 KB)
Trainable params: 85889 (335.50 KB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
None
====================================================================================================
Successfully loaded fold 1 data
Train size:  (160, 192, 256, 3)
Val size:  (40, 192, 256, 3)
====================================================================================================
Epoch 1/20
9/9 - 6s - loss: 0.6203 - compute_iou: 0.2404 - val_loss: 0.5463 - val_compute_iou: 0.2303 - 6s/epoch - 654ms/step
Epoch 2/20
9/9 - 3s - loss: 0.5345 - compute_iou: 0.2275 - val_loss: 0.5187 - val_compute_iou: 0.2633 - 3s/epoch - 363ms/step
Epoch 3/20
9/9 - 3s - loss: 0.5037 - compute_iou: 0.2573 - val_loss: 0.4775 - val_compute_iou: 0.2675 - 3s/epoch - 371ms/step
Epoch 4/20
9/9 - 3s - loss: 0.4700 - compute_iou: 0.2918 - val_loss: 0.4641 - val_compute_iou: 0.2869 - 3s/epoch - 359ms/step
Epoch 5/20
9/9 - 3s - loss: 0.4522 - compute_iou: 0.3108 - val_loss: 0.4583 - val_compute_iou: 0.3154 - 3s/epoch - 360ms/step
Epoch 6/20
9/9 - 3s - loss: 0.4414 - compute_iou: 0.3246 - val_loss: 0.4398 - val_compute_iou: 0.3254 - 3s/epoch - 353ms/step
Epoch 7/20
9/9 - 3s - loss: 0.4245 - compute_iou: 0.3441 - val_loss: 0.4110 - val_compute_iou: 0.3502 - 3s/epoch - 359ms/step
Epoch 8/20
9/9 - 3s - loss: 0.4013 - compute_iou: 0.3822 - val_loss: 0.3466 - val_compute_iou: 0.4495 - 3s/epoch - 356ms/step
Epoch 9/20
9/9 - 3s - loss: 0.4228 - compute_iou: 0.4622 - val_loss: 0.4066 - val_compute_iou: 0.3797 - 3s/epoch - 355ms/step
Epoch 10/20
9/9 - 3s - loss: 0.4099 - compute_iou: 0.3755 - val_loss: 0.4011 - val_compute_iou: 0.3769 - 3s/epoch - 354ms/step
Epoch 11/20
9/9 - 3s - loss: 0.3849 - compute_iou: 0.3871 - val_loss: 0.3451 - val_compute_iou: 0.4330 - 3s/epoch - 356ms/step
Epoch 12/20
9/9 - 3s - loss: 0.3342 - compute_iou: 0.4784 - val_loss: 0.3109 - val_compute_iou: 0.5515 - 3s/epoch - 351ms/step
Epoch 13/20
9/9 - 3s - loss: 0.3597 - compute_iou: 0.5039 - val_loss: 0.2800 - val_compute_iou: 0.5519 - 3s/epoch - 355ms/step
Epoch 14/20
9/9 - 3s - loss: 0.3028 - compute_iou: 0.5660 - val_loss: 0.2580 - val_compute_iou: 0.5870 - 3s/epoch - 354ms/step
Epoch 15/20
9/9 - 3s - loss: 0.2838 - compute_iou: 0.5801 - val_loss: 0.2415 - val_compute_iou: 0.6068 - 3s/epoch - 357ms/step
Epoch 16/20
9/9 - 3s - loss: 0.2833 - compute_iou: 0.5977 - val_loss: 0.2587 - val_compute_iou: 0.6462 - 3s/epoch - 358ms/step
Epoch 17/20
9/9 - 3s - loss: 0.2681 - compute_iou: 0.6026 - val_loss: 0.2362 - val_compute_iou: 0.6378 - 3s/epoch - 360ms/step
Epoch 18/20
9/9 - 3s - loss: 0.2481 - compute_iou: 0.6348 - val_loss: 0.2532 - val_compute_iou: 0.6901 - 3s/epoch - 359ms/step
Epoch 19/20
9/9 - 3s - loss: 0.2703 - compute_iou: 0.6029 - val_loss: 0.2265 - val_compute_iou: 0.6471 - 3s/epoch - 356ms/step
Epoch 20/20
9/9 - 3s - loss: 0.2674 - compute_iou: 0.6081 - val_loss: 0.2125 - val_compute_iou: 0.6747 - 3s/epoch - 359ms/step
1/2 [==============>...............] - ETA: 0s2/2 [==============================] - 0s 29ms/step
Fold 1 - IoU 0.6452153325080872 - Loss [0.6202710866928101, 0.534528911113739, 0.5036947727203369, 0.4699578881263733, 0.452200710773468, 0.4414297938346863, 0.42448291182518005, 0.4013465344905853, 0.4227907061576843, 0.409900426864624, 0.3848707973957062, 0.33424270153045654, 0.3596620261669159, 0.3028191924095154, 0.2837953269481659, 0.283270925283432, 0.26810744404792786, 0.24814005196094513, 0.2702590525150299, 0.26743587851524353]
====================================================================================================
Successfully loaded fold 2 data
Train size:  (160, 192, 256, 3)
Val size:  (40, 192, 256, 3)
====================================================================================================
Epoch 1/20
9/9 - 6s - loss: 0.6499 - compute_iou: 0.2385 - val_loss: 0.5873 - val_compute_iou: 0.2209 - 6s/epoch - 633ms/step
Epoch 2/20
9/9 - 3s - loss: 0.5913 - compute_iou: 0.2047 - val_loss: 0.5540 - val_compute_iou: 0.2061 - 3s/epoch - 360ms/step
Epoch 3/20
9/9 - 3s - loss: 0.5645 - compute_iou: 0.2194 - val_loss: 0.5321 - val_compute_iou: 0.2146 - 3s/epoch - 361ms/step
Epoch 4/20
9/9 - 3s - loss: 0.5351 - compute_iou: 0.2355 - val_loss: 0.4848 - val_compute_iou: 0.2371 - 3s/epoch - 362ms/step
Epoch 5/20
9/9 - 3s - loss: 0.5012 - compute_iou: 0.2657 - val_loss: 0.4516 - val_compute_iou: 0.2515 - 3s/epoch - 357ms/step
Epoch 6/20
9/9 - 3s - loss: 0.4722 - compute_iou: 0.2934 - val_loss: 0.4308 - val_compute_iou: 0.2750 - 3s/epoch - 362ms/step
Epoch 7/20
9/9 - 3s - loss: 0.4558 - compute_iou: 0.3081 - val_loss: 0.4119 - val_compute_iou: 0.2989 - 3s/epoch - 365ms/step
Epoch 8/20
9/9 - 3s - loss: 0.4465 - compute_iou: 0.3202 - val_loss: 0.4094 - val_compute_iou: 0.3171 - 3s/epoch - 367ms/step
Epoch 9/20
9/9 - 3s - loss: 0.4241 - compute_iou: 0.3345 - val_loss: 0.4173 - val_compute_iou: 0.3241 - 3s/epoch - 366ms/step
Epoch 10/20
9/9 - 3s - loss: 0.4212 - compute_iou: 0.3396 - val_loss: 0.3961 - val_compute_iou: 0.3092 - 3s/epoch - 362ms/step
Epoch 11/20
9/9 - 3s - loss: 0.4155 - compute_iou: 0.3407 - val_loss: 0.4040 - val_compute_iou: 0.3085 - 3s/epoch - 363ms/step
Epoch 12/20
9/9 - 3s - loss: 0.4142 - compute_iou: 0.3487 - val_loss: 0.3574 - val_compute_iou: 0.3556 - 3s/epoch - 361ms/step
Epoch 13/20
9/9 - 3s - loss: 0.4034 - compute_iou: 0.3577 - val_loss: 0.3541 - val_compute_iou: 0.3597 - 3s/epoch - 359ms/step
Epoch 14/20
9/9 - 3s - loss: 0.4084 - compute_iou: 0.3490 - val_loss: 0.3548 - val_compute_iou: 0.3488 - 3s/epoch - 358ms/step
Epoch 15/20
9/9 - 3s - loss: 0.3788 - compute_iou: 0.3699 - val_loss: 0.3460 - val_compute_iou: 0.3673 - 3s/epoch - 361ms/step
Epoch 16/20
9/9 - 3s - loss: 0.3698 - compute_iou: 0.3847 - val_loss: 0.3311 - val_compute_iou: 0.3880 - 3s/epoch - 362ms/step
Epoch 17/20
9/9 - 3s - loss: 0.3617 - compute_iou: 0.3930 - val_loss: 0.3282 - val_compute_iou: 0.3851 - 3s/epoch - 364ms/step
Epoch 18/20
9/9 - 3s - loss: 0.3600 - compute_iou: 0.3927 - val_loss: 0.3213 - val_compute_iou: 0.3868 - 3s/epoch - 356ms/step
Epoch 19/20
9/9 - 3s - loss: 0.3467 - compute_iou: 0.4041 - val_loss: 0.3228 - val_compute_iou: 0.3977 - 3s/epoch - 357ms/step
Epoch 20/20
9/9 - 3s - loss: 0.3315 - compute_iou: 0.4180 - val_loss: 0.2989 - val_compute_iou: 0.4105 - 3s/epoch - 357ms/step
1/2 [==============>...............] - ETA: 0s2/2 [==============================] - 0s 25ms/step
Fold 2 - IoU 0.42491766810417175 - Loss [0.6499049067497253, 0.5912854671478271, 0.5644659399986267, 0.5351347923278809, 0.5012006163597107, 0.4722047448158264, 0.4558177888393402, 0.44652777910232544, 0.42409762740135193, 0.421154260635376, 0.41547122597694397, 0.41420841217041016, 0.4034234583377838, 0.4083986282348633, 0.3788074553012848, 0.369770348072052, 0.3616786599159241, 0.35996198654174805, 0.3467303216457367, 0.3314727246761322]
====================================================================================================
Successfully loaded fold 3 data
Train size:  (160, 192, 256, 3)
Val size:  (40, 192, 256, 3)
====================================================================================================
Epoch 1/20
9/9 - 6s - loss: 0.6431 - compute_iou: 0.2417 - val_loss: 0.5514 - val_compute_iou: 0.2091 - 6s/epoch - 616ms/step
Epoch 2/20
9/9 - 3s - loss: 0.5819 - compute_iou: 0.2071 - val_loss: 0.5230 - val_compute_iou: 0.2056 - 3s/epoch - 358ms/step
Epoch 3/20
9/9 - 3s - loss: 0.5625 - compute_iou: 0.2219 - val_loss: 0.5067 - val_compute_iou: 0.2204 - 3s/epoch - 358ms/step
Epoch 4/20
9/9 - 3s - loss: 0.5400 - compute_iou: 0.2290 - val_loss: 0.5045 - val_compute_iou: 0.2550 - 3s/epoch - 359ms/step
Epoch 5/20
9/9 - 3s - loss: 0.5052 - compute_iou: 0.2744 - val_loss: 0.4127 - val_compute_iou: 0.3151 - 3s/epoch - 358ms/step
Epoch 6/20
9/9 - 3s - loss: 0.4075 - compute_iou: 0.3786 - val_loss: 0.2986 - val_compute_iou: 0.5119 - 3s/epoch - 357ms/step
Epoch 7/20
9/9 - 3s - loss: 0.3547 - compute_iou: 0.5237 - val_loss: 0.2811 - val_compute_iou: 0.5794 - 3s/epoch - 355ms/step
Epoch 8/20
9/9 - 3s - loss: 0.3449 - compute_iou: 0.5139 - val_loss: 0.2794 - val_compute_iou: 0.5301 - 3s/epoch - 364ms/step
Epoch 9/20
9/9 - 3s - loss: 0.3163 - compute_iou: 0.5306 - val_loss: 0.2614 - val_compute_iou: 0.5893 - 3s/epoch - 363ms/step
Epoch 10/20
9/9 - 3s - loss: 0.3027 - compute_iou: 0.5593 - val_loss: 0.2597 - val_compute_iou: 0.5758 - 3s/epoch - 365ms/step
Epoch 11/20
9/9 - 3s - loss: 0.3182 - compute_iou: 0.5417 - val_loss: 0.2377 - val_compute_iou: 0.6059 - 3s/epoch - 360ms/step
Epoch 12/20
9/9 - 3s - loss: 0.3310 - compute_iou: 0.5327 - val_loss: 0.2734 - val_compute_iou: 0.5919 - 3s/epoch - 364ms/step
Epoch 13/20
9/9 - 3s - loss: 0.2901 - compute_iou: 0.5432 - val_loss: 0.2759 - val_compute_iou: 0.5892 - 3s/epoch - 360ms/step
Epoch 14/20
9/9 - 3s - loss: 0.2945 - compute_iou: 0.5675 - val_loss: 0.2658 - val_compute_iou: 0.6206 - 3s/epoch - 360ms/step
Epoch 15/20
9/9 - 3s - loss: 0.3088 - compute_iou: 0.5663 - val_loss: 0.2740 - val_compute_iou: 0.6239 - 3s/epoch - 358ms/step
Epoch 16/20
9/9 - 3s - loss: 0.2989 - compute_iou: 0.5717 - val_loss: 0.2469 - val_compute_iou: 0.6060 - 3s/epoch - 353ms/step
Epoch 17/20
9/9 - 3s - loss: 0.2631 - compute_iou: 0.6055 - val_loss: 0.2260 - val_compute_iou: 0.6336 - 3s/epoch - 356ms/step
Epoch 18/20
9/9 - 3s - loss: 0.2612 - compute_iou: 0.6154 - val_loss: 0.2139 - val_compute_iou: 0.6675 - 3s/epoch - 361ms/step
Epoch 19/20
9/9 - 3s - loss: 0.2635 - compute_iou: 0.6196 - val_loss: 0.2135 - val_compute_iou: 0.6487 - 3s/epoch - 363ms/step
Epoch 20/20
9/9 - 3s - loss: 0.2570 - compute_iou: 0.6240 - val_loss: 0.2334 - val_compute_iou: 0.6417 - 3s/epoch - 361ms/step
1/2 [==============>...............] - ETA: 0s2/2 [==============================] - 0s 29ms/step
Fold 3 - IoU 0.6219572424888611 - Loss [0.6431167721748352, 0.5819041132926941, 0.562500536441803, 0.5400280952453613, 0.5051668882369995, 0.40753859281539917, 0.35465162992477417, 0.3448787033557892, 0.3162868022918701, 0.30267220735549927, 0.31817275285720825, 0.3310415744781494, 0.29005783796310425, 0.2944920063018799, 0.3088209927082062, 0.2989441454410553, 0.26311805844306946, 0.26116058230400085, 0.2634592652320862, 0.2570120692253113]
====================================================================================================
Successfully loaded fold 4 data
Train size:  (160, 192, 256, 3)
Val size:  (40, 192, 256, 3)
====================================================================================================
Epoch 1/20
9/9 - 6s - loss: 0.6500 - compute_iou: 0.2267 - val_loss: 0.6546 - val_compute_iou: 0.2498 - 6s/epoch - 641ms/step
Epoch 2/20
9/9 - 3s - loss: 0.5646 - compute_iou: 0.1890 - val_loss: 0.6611 - val_compute_iou: 0.2255 - 3s/epoch - 361ms/step
Epoch 3/20
9/9 - 3s - loss: 0.5372 - compute_iou: 0.2138 - val_loss: 0.6124 - val_compute_iou: 0.2644 - 3s/epoch - 360ms/step
Epoch 4/20
9/9 - 3s - loss: 0.5100 - compute_iou: 0.2316 - val_loss: 0.5832 - val_compute_iou: 0.2954 - 3s/epoch - 360ms/step
Epoch 5/20
9/9 - 3s - loss: 0.4790 - compute_iou: 0.2664 - val_loss: 0.6015 - val_compute_iou: 0.2719 - 3s/epoch - 359ms/step
Epoch 6/20
9/9 - 3s - loss: 0.4619 - compute_iou: 0.2842 - val_loss: 0.5500 - val_compute_iou: 0.3231 - 3s/epoch - 362ms/step
Epoch 7/20
9/9 - 3s - loss: 0.4725 - compute_iou: 0.3363 - val_loss: 0.5205 - val_compute_iou: 0.4014 - 3s/epoch - 367ms/step
Epoch 8/20
9/9 - 3s - loss: 0.4373 - compute_iou: 0.3304 - val_loss: 0.4817 - val_compute_iou: 0.3684 - 3s/epoch - 364ms/step
Epoch 9/20
9/9 - 3s - loss: 0.3554 - compute_iou: 0.4224 - val_loss: 0.3880 - val_compute_iou: 0.5260 - 3s/epoch - 369ms/step
Epoch 10/20
9/9 - 3s - loss: 0.3092 - compute_iou: 0.5790 - val_loss: 0.3799 - val_compute_iou: 0.5176 - 3s/epoch - 368ms/step
Epoch 11/20
9/9 - 3s - loss: 0.2783 - compute_iou: 0.5638 - val_loss: 0.3895 - val_compute_iou: 0.5021 - 3s/epoch - 361ms/step
Epoch 12/20
9/9 - 3s - loss: 0.2734 - compute_iou: 0.5815 - val_loss: 0.3620 - val_compute_iou: 0.5500 - 3s/epoch - 359ms/step
Epoch 13/20
9/9 - 3s - loss: 0.2501 - compute_iou: 0.6343 - val_loss: 0.4148 - val_compute_iou: 0.4948 - 3s/epoch - 361ms/step
Epoch 14/20
9/9 - 3s - loss: 0.2317 - compute_iou: 0.6368 - val_loss: 0.3601 - val_compute_iou: 0.5458 - 3s/epoch - 361ms/step
Epoch 15/20
9/9 - 3s - loss: 0.2240 - compute_iou: 0.6478 - val_loss: 0.2928 - val_compute_iou: 0.6448 - 3s/epoch - 363ms/step
Epoch 16/20
9/9 - 3s - loss: 0.2018 - compute_iou: 0.6785 - val_loss: 0.3133 - val_compute_iou: 0.6613 - 3s/epoch - 362ms/step
Epoch 17/20
9/9 - 3s - loss: 0.2109 - compute_iou: 0.6932 - val_loss: 0.2697 - val_compute_iou: 0.6457 - 3s/epoch - 356ms/step
Epoch 18/20
9/9 - 3s - loss: 0.2059 - compute_iou: 0.6910 - val_loss: 0.4845 - val_compute_iou: 0.4635 - 3s/epoch - 360ms/step
Epoch 19/20
9/9 - 3s - loss: 0.3118 - compute_iou: 0.5257 - val_loss: 0.4362 - val_compute_iou: 0.4307 - 3s/epoch - 355ms/step
Epoch 20/20
9/9 - 3s - loss: 0.2965 - compute_iou: 0.5380 - val_loss: 0.3875 - val_compute_iou: 0.5143 - 3s/epoch - 358ms/step
1/2 [==============>...............] - ETA: 0s2/2 [==============================] - 0s 25ms/step
Fold 4 - IoU 0.5306848883628845 - Loss [0.650040328502655, 0.5646371245384216, 0.5371920466423035, 0.5100060701370239, 0.4789511561393738, 0.461904913187027, 0.4724869728088379, 0.4372561573982239, 0.3554154932498932, 0.30916285514831543, 0.27826377749443054, 0.27341538667678833, 0.2500675618648529, 0.2316526472568512, 0.2239626944065094, 0.20175352692604065, 0.21093186736106873, 0.20586872100830078, 0.31180712580680847, 0.2965434491634369]
====================================================================================================
Successfully loaded fold 5 data
Train size:  (160, 192, 256, 3)
Val size:  (40, 192, 256, 3)
====================================================================================================
Epoch 1/20
9/9 - 6s - loss: 0.6468 - compute_iou: 0.2415 - val_loss: 0.5878 - val_compute_iou: 0.2242 - 6s/epoch - 618ms/step
Epoch 2/20
9/9 - 3s - loss: 0.5833 - compute_iou: 0.2100 - val_loss: 0.5490 - val_compute_iou: 0.1998 - 3s/epoch - 362ms/step
Epoch 3/20
9/9 - 3s - loss: 0.5723 - compute_iou: 0.2036 - val_loss: 0.5421 - val_compute_iou: 0.2131 - 3s/epoch - 358ms/step
Epoch 4/20
9/9 - 3s - loss: 0.5633 - compute_iou: 0.2181 - val_loss: 0.5340 - val_compute_iou: 0.2208 - 3s/epoch - 356ms/step
Epoch 5/20
9/9 - 3s - loss: 0.5505 - compute_iou: 0.2242 - val_loss: 0.5118 - val_compute_iou: 0.2355 - 3s/epoch - 356ms/step
Epoch 6/20
9/9 - 3s - loss: 0.5033 - compute_iou: 0.2595 - val_loss: 0.4385 - val_compute_iou: 0.3075 - 3s/epoch - 356ms/step
Epoch 7/20
9/9 - 3s - loss: 0.4620 - compute_iou: 0.3062 - val_loss: 0.4184 - val_compute_iou: 0.3309 - 3s/epoch - 362ms/step
Epoch 8/20
9/9 - 3s - loss: 0.4364 - compute_iou: 0.3264 - val_loss: 0.4311 - val_compute_iou: 0.3395 - 3s/epoch - 361ms/step
Epoch 9/20
9/9 - 3s - loss: 0.4199 - compute_iou: 0.3681 - val_loss: 0.3487 - val_compute_iou: 0.4553 - 3s/epoch - 361ms/step
Epoch 10/20
9/9 - 3s - loss: 0.3430 - compute_iou: 0.4888 - val_loss: 0.3151 - val_compute_iou: 0.5637 - 3s/epoch - 360ms/step
Epoch 11/20
9/9 - 3s - loss: 0.3126 - compute_iou: 0.5706 - val_loss: 0.3217 - val_compute_iou: 0.5729 - 3s/epoch - 358ms/step
Epoch 12/20
9/9 - 3s - loss: 0.3481 - compute_iou: 0.5227 - val_loss: 0.2808 - val_compute_iou: 0.5822 - 3s/epoch - 362ms/step
Epoch 13/20
9/9 - 3s - loss: 0.2680 - compute_iou: 0.5958 - val_loss: 0.2395 - val_compute_iou: 0.6659 - 3s/epoch - 357ms/step
Epoch 14/20
9/9 - 3s - loss: 0.2522 - compute_iou: 0.6216 - val_loss: 0.2608 - val_compute_iou: 0.6678 - 3s/epoch - 362ms/step
Epoch 15/20
9/9 - 3s - loss: 0.2321 - compute_iou: 0.6707 - val_loss: 0.2318 - val_compute_iou: 0.7020 - 3s/epoch - 365ms/step
Epoch 16/20
9/9 - 3s - loss: 0.2255 - compute_iou: 0.6781 - val_loss: 0.2361 - val_compute_iou: 0.6796 - 3s/epoch - 363ms/step
Epoch 17/20
9/9 - 3s - loss: 0.2241 - compute_iou: 0.6589 - val_loss: 0.2224 - val_compute_iou: 0.7065 - 3s/epoch - 363ms/step
Epoch 18/20
9/9 - 3s - loss: 0.2096 - compute_iou: 0.6962 - val_loss: 0.2126 - val_compute_iou: 0.7139 - 3s/epoch - 362ms/step
Epoch 19/20
9/9 - 3s - loss: 0.2281 - compute_iou: 0.6693 - val_loss: 0.2269 - val_compute_iou: 0.6942 - 3s/epoch - 364ms/step
Epoch 20/20
9/9 - 3s - loss: 0.2109 - compute_iou: 0.6855 - val_loss: 0.2463 - val_compute_iou: 0.7058 - 3s/epoch - 366ms/step
1/2 [==============>...............] - ETA: 0s2/2 [==============================] - 0s 26ms/step
Fold 5 - IoU 0.6685610413551331 - Loss [0.6468297243118286, 0.5833325386047363, 0.5722872018814087, 0.5632634162902832, 0.550471305847168, 0.5033033490180969, 0.4620346426963806, 0.43635183572769165, 0.4198846220970154, 0.3429970443248749, 0.31261080503463745, 0.3481016755104065, 0.26803284883499146, 0.2522292733192444, 0.2321304827928543, 0.22546610236167908, 0.22411692142486572, 0.2096165418624878, 0.22812528908252716, 0.21090659499168396]
====================================================================================================
====================================================================================================
Average IoU: 0.5783
====================================================================================================
====================================================================================================
